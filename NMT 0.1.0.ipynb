{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "import nltk\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "# Seq2Seq Items\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.rnn import DropoutWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 31411\n",
    "num_units = 256\n",
    "input_size = 128\n",
    "batch_size = 128\n",
    "source_sequence_length=40\n",
    "target_sequence_length=60\n",
    "decoder_type = 'luong-attention' # could be luong or Bahdanau styled attenion\n",
    "sentences_to_read = 10\n",
    "cell_type='LSTM'\n",
    "embedding_dimensions = 512\n",
    "encoder_type='bi_directional'\n",
    "beam_search = 'true'\n",
    "beam_width = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), (',', 3), ('.', 4), ('và', 5), ('tôi', 6), ('là', 7), ('một', 8), ('những', 9)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'và'), (6, 'tôi'), (7, 'là'), (8, 'một'), (9, 'những')]\n",
      "\t Vocabulary size:  31411\n",
      "Target\n",
      "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), (',', 3), ('.', 4), ('the', 5), ('and', 6), ('to', 7), ('of', 8), ('a', 9)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'the'), (6, 'and'), (7, 'to'), (8, 'of'), (9, 'a')]\n",
      "\t Vocabulary size:  31411\n"
     ]
    }
   ],
   "source": [
    "# Loading Vocabulary from vocabulary files\n",
    "src_dictionary = dict()\n",
    "with open('vocab.vi.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        src_dictionary[line[:-1]] = len(src_dictionary) # Assign an ID to each word in dictionary, ID is currently the entry and the word is the key\n",
    "\n",
    "src_reverse_dictionary = dict(zip(src_dictionary.values(),src_dictionary.keys())) # Reverse order, now ID is the key and the word is the entry\n",
    "#Somehow zipping the two list rearranges the zip to be in order.\n",
    "\n",
    "print('Source')\n",
    "print('\\t',list(src_dictionary.items())[:10]) \n",
    "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(src_dictionary))\n",
    "\n",
    "tgt_dictionary = dict()\n",
    "with open('vocab.en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
    "\n",
    "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
    "\n",
    "print('Target')\n",
    "print('\\t',list(tgt_dictionary.items())[:10])\n",
    "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(tgt_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loading Sentences(English and Vietnamese)\n",
    "source_sent = []\n",
    "target_sent = []\n",
    "\n",
    "test_source_sent = []\n",
    "test_target_sent = []\n",
    "\n",
    "'''\n",
    "with open('train.en.txt', encoding='utf-8') as f_en:\n",
    "    for l_i, line in enumerate(f_en):\n",
    "        target_sent.append(line)\n",
    "        if len(target_sent)>=sentences_to_read:\n",
    "            break\n",
    "            \n",
    "with open('train.vi.txt', encoding='utf-8') as f_vi:\n",
    "    for l_i, line in enumerate(f_vi):\n",
    "        source_sent.append(line)\n",
    "        if len(source_sent)>=sentences_to_read:\n",
    "            break\n",
    "'''            \n",
    "with open('test.vi.txt',encoding='utf-8') as f_vi_tst:\n",
    "    for idx, line in enumerate(f_vi_tst):\n",
    "        sent = word_tokenize(line)\n",
    "        test_source_sent.append(sent)\n",
    "        if len(test_source_sent)>= batch_size:\n",
    "            break\n",
    "        \n",
    "# assert len(source_sent)==len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_tokens(sent,is_source):\n",
    "    #sent = sent.replace('-',' ')\n",
    "    sent = sent.replace(',',' ,')\n",
    "    sent = sent.replace('.',' .')\n",
    "    sent = sent.replace('\\n',' ') \n",
    "    \n",
    "    sent_toks = sent.split(' ')\n",
    "    for t_i, tok in enumerate(sent_toks):\n",
    "        if is_source:\n",
    "            if tok not in src_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "        else:\n",
    "            if tok not in tgt_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "    return sent_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs=[]\n",
    "train_inp_lengths=[]\n",
    "\n",
    "src_max_sent_length= 41\n",
    "tgt_max_sent_length= 61\n",
    "\n",
    "for s_i,src_sent in enumerate (test_source_sent):\n",
    "    src_sentence = ' '.join(src_sent)\n",
    "    src_sent_tokens= split_to_tokens(src_sentence,True)\n",
    "    \n",
    "    #Process sentences for batch training(ADD special tokens and MAKE sentences into SAME length)\n",
    "    #Source Language\n",
    "    ############################################\n",
    "    num_src_sent = []\n",
    "    for tok in src_sent_tokens:\n",
    "        num_src_sent.append(src_dictionary[tok])\n",
    "    \n",
    "    num_src_rvs= num_src_sent[::-1]\n",
    "    num_src_sent.insert(0,src_dictionary['<s>'])\n",
    "    \n",
    "    train_inp_lengths.append(min(src_max_sent_length, len(num_src_sent)+1))\n",
    "    if len(num_src_sent)<src_max_sent_length:\n",
    "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
    "    # if more than max length, truncate the sentence\n",
    "    elif len(num_src_sent)>src_max_sent_length:\n",
    "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
    "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
    "    train_inputs.append(num_src_sent)\n",
    "  \n",
    "#for i in train_inputs[1]:\n",
    "    #print(src_reverse_dictionary[i])\n",
    "    \n",
    "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
    "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data\n",
      "['xin', 'tôi']\n",
      "['chào', 'tên']\n",
      "['</s>', 'là']\n",
      "['</s>', 'john']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n",
      "['</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "input_size = 128\n",
    "\n",
    "class DataGeneratorMT(object):\n",
    "    \n",
    "    def __init__(self,batch_size,num_unroll,is_source):\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "        \n",
    "        self._src_word_embeddings = np.load('vi_embeddings.npy')\n",
    "        \n",
    "        self._tgt_word_embeddings = np.load('en_embeddings.npy')\n",
    "        \n",
    "        self._sent_ids = None\n",
    "        \n",
    "        self._is_source = is_source\n",
    "        \n",
    "                \n",
    "    def next_batch(self, sent_ids, first_set):\n",
    "        \n",
    "        if self._is_source:\n",
    "            max_sent_length = src_max_sent_length\n",
    "        else:\n",
    "            max_sent_length = tgt_max_sent_length\n",
    "        batch_labels_ind = []\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            \n",
    "            sent_id = sent_ids[b]\n",
    "            \n",
    "            if self._is_source:\n",
    "                sent_text = train_inputs[sent_id]\n",
    "                             \n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b]=sent_text[self._cursor[b]+1]\n",
    "\n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id]\n",
    "                \n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b]+1]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
    "                                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self,sent_ids):\n",
    "        \n",
    "        if sent_ids is not None:\n",
    "            \n",
    "            self._sent_ids = sent_ids\n",
    "            \n",
    "            self._cursor = [0 for _ in range(self._batch_size)]\n",
    "                \n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        inp_lengths = []\n",
    "        for ui in range(self._num_unroll):\n",
    "            \n",
    "            data, labels = self.next_batch(self._sent_ids, False)\n",
    "                    \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            \n",
    "        for x in sent_ids:\n",
    "            inp_lengths.append(train_inp_lengths[x])\n",
    "            \n",
    "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if the implementation correct\n",
    "dg = DataGeneratorMT(batch_size=2,num_unroll=40,is_source=True)\n",
    "u_data, u_labels, _, _ = dg.unroll_batches([0,1])\n",
    "\n",
    "print('Source data')\n",
    "for _, lbl in zip(u_data,u_labels):\n",
    "    print([src_reverse_dictionary[w] for w in lbl.tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Initial Encoding and Decoding mask layer\n",
    "enc_train_inputs= []\n",
    "\n",
    "#Embedding Layer, received from pre-built word2vec embedding\n",
    "encoder_emb_layer = tf.convert_to_tensor(np.load('vi_embeddings.npy'),dtype=tf.float32)\n",
    "decoder_emb_layer = tf.convert_to_tensor(np.load('en_embeddings.npy'),dtype=tf.float32)\n",
    "\n",
    "#Defined unrolled training inputs\n",
    "for ui in range(source_sequence_length):\n",
    "    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size], name='enc_train_inputs_%d'%ui))\n",
    "    \n",
    "encoder_emb_inp=[tf.nn.embedding_lookup(encoder_emb_layer,src) for src in enc_train_inputs]\n",
    "encoder_emb_inp= tf.stack(encoder_emb_inp)\n",
    "\n",
    "enc_train_inp_lengths= tf.placeholder(tf.int32, shape=[batch_size], name= \"train_input_lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder\n",
    "if encoder_type == 'uni_directional':\n",
    "    encoder_cell= tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "    initial_state= encoder_cell.zero_state(batch_size, dtype= tf.float32)\n",
    "\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp, initial_state=initial_state,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True, swap_memory=True)\n",
    "\n",
    "else:\n",
    "    forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "    #forward_cell = DropoutWrapper(forward_cell, input_keep_prob = 0.8)\n",
    "    #backward_cell = DropoutWrapper(backward_cell, input_keep_prob = 0.8)\n",
    "\n",
    "    initial_state_fw= forward_cell.zero_state(batch_size, dtype= tf.float32)\n",
    "    initial_state_bw= backward_cell.zero_state(batch_size, dtype= tf.float32)\n",
    "\n",
    "    bi_outputs, encoder_final_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, encoder_emb_inp, initial_state_fw=initial_state_fw,\n",
    "    initial_state_bw=initial_state_bw,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True)\n",
    "    \n",
    "    encoder_outputs = tf.concat(bi_outputs,-1)\n",
    "    \n",
    "encoder_outputs= tf.transpose(encoder_outputs,[1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Bahdanau Attention\n",
    "# @attention_states=@memory in documentation  [batch_size, max_time, num_units]\n",
    "#Define Decoder for Training\n",
    "tgt_sos_id = 1\n",
    "tgt_eos_id = 2\n",
    "\n",
    "if encoder_type == 'uni_directional':\n",
    "    decoder_cell= tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "else:\n",
    "    cells = [tf.nn.rnn_cell.BasicLSTMCell(num_units),tf.nn.rnn_cell.BasicLSTMCell(num_units)]\n",
    "    decoder_cell= tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "projection_layer= Dense(units=vocab_size,use_bias=True)\n",
    "\n",
    "\n",
    "if beam_search:\n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
    "    encoder_outputs, multiplier=beam_width)\n",
    "    \n",
    "    tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
    "    encoder_final_state, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism= tf.contrib.seq2seq.LuongAttention(num_units, tiled_encoder_outputs,scale=True)\n",
    "    attention_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,attention_layer_size=num_units) \n",
    "    \n",
    "    decoder_initial_state = attention_cell.zero_state(dtype = tf.float32, batch_size= batch_size * beam_width).clone(cell_state=tiled_encoder_final_state)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=attention_cell,\n",
    "        embedding=decoder_emb_layer,\n",
    "        start_tokens=tf.fill([batch_size], tgt_sos_id),\n",
    "        end_token=tgt_eos_id,\n",
    "        initial_state=decoder_initial_state,\n",
    "        beam_width=beam_width,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0)\n",
    "    \n",
    "    outputs_test, _,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True\n",
    "    )\n",
    "    translations = outputs_test.predicted_ids\n",
    "    translations = tf.transpose(translations, perm=[1, 2, 0])\n",
    "\n",
    "else:\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding = decoder_emb_layer,\n",
    "        start_tokens = tf.fill([batch_size], tgt_sos_id), \n",
    "        end_token = tgt_eos_id)\n",
    "\n",
    "\n",
    "    maximum_iterations = tf.round(tf.reduce_max(enc_train_inp_lengths) * 2)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,attention_layer_size=num_units)\n",
    "    \n",
    "    attention_states = encoder_outputs\n",
    "    attention_mechanism= tf.contrib.seq2seq.LuongAttention(num_units, attention_states,scale=True)\n",
    "    initial_state= decoder_cell.zero_state(dtype=tf.float32, batch_size= batch_size).clone(cell_state=encoder_state)\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, initial_state,\n",
    "        output_layer=projection_layer)\n",
    "\n",
    "    # Get Outputs from Decoder\n",
    "    outputs_test, _,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True,\n",
    "    maximum_iterations = maximum_iterations\n",
    "    )\n",
    "    \n",
    "    translations = outputs_test.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "(128, 10, 21)\n",
      "Predicted: hi  .   \n",
      "\n",
      "\n",
      "\n",
      "Predicted: my name is john  .   \n",
      "\n",
      "\n",
      "\n",
      "Predicted: i like to eat a dog  .   \n",
      "\n",
      "\n",
      "\n",
      "Predicted: the day  ,  i went to dinner with my mother at home  .    \n",
      "\n",
      "\n",
      "\n",
      "Predicted:  likes to smoke  .    \n",
      "\n",
      "\n",
      "\n",
      "Predicted: the robot is a very interesting robot  .   \n",
      "\n",
      "\n",
      "\n",
      "Predicted: i am going to the door  .   \n",
      "\n",
      "\n",
      "\n",
      "Predicted: he goes out  .    \n",
      "\n",
      "\n",
      "\n",
      "Predicted: her mother had a cat  ,  a dog and a mouse  .    \n",
      "\n",
      "\n",
      "\n",
      "Predicted: i was 16  .    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #for tensor in tf.get_default_graph().as_graph_def().node:\n",
    "        #print (str(tensor.name))\n",
    "   \n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "    for var_train in tf.trainable_variables():\n",
    "        name = var_train.name\n",
    "        # print(name)\n",
    "        arr_init = tf.train.load_variable('./models/Attention-Bi-Tokenized-292K-ViEn-B64-Luong-98000',name)\n",
    "        var_train.load(arr_init, session=sess)\n",
    "        \n",
    "    print('Done')\n",
    "\n",
    "    bleu_scores_over_time = []\n",
    "\n",
    "    src_word_embeddings = np.load('vi_embeddings.npy')\n",
    "    tgt_word_embeddings = np.load('en_embeddings.npy')\n",
    "\n",
    "    enc_data_generator = DataGeneratorMT(batch_size = batch_size, num_unroll=source_sequence_length,is_source=True)\n",
    "    # input_sizes for each bin: [40]\n",
    "    # output_sizes for each bin: [60]\n",
    "        \n",
    "    # ====================== ENCODER DATA COLLECTION ================================================\n",
    "    inp = []\n",
    "    for x in range(128):\n",
    "        inp.append(x)\n",
    "    eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(inp)\n",
    "    feed_dict = {}\n",
    "    \n",
    "    #print(len(eu_lengths))\n",
    "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "    \n",
    "    for ui, dat in enumerate(eu_data):            \n",
    "        feed_dict[enc_train_inputs[ui]] = dat\n",
    "    \n",
    "    # ======================= OPTIMIZATION ==========================\n",
    "   \n",
    "    \n",
    "    tr_pred = sess.run(translations, feed_dict=feed_dict)\n",
    "    bleu_labels, bleu_preds = [],[]\n",
    "        \n",
    "    print('Done')\n",
    "    print(tr_pred.shape)\n",
    "    \n",
    "    for i in range(sentences_to_read):\n",
    "        print_str = 'Predicted: '\n",
    "        for j in range(10):\n",
    "            end = False\n",
    "            for k in range (21):\n",
    "                print_str += tgt_reverse_dictionary[tr_pred[i][j][k]] + ' '\n",
    "                \n",
    "                if tgt_reverse_dictionary[tr_pred[i][j][k]] == '</s>':\n",
    "                    end = True\n",
    "                    break\n",
    "                    \n",
    "            if (end): \n",
    "                break\n",
    "        \n",
    "                \n",
    "        print(print_str.replace('<unk>','').replace('</s>',''))\n",
    "        print()\n",
    "        print('\\n') \n",
    "    #sess.run(reset_train_state) # resetting hidden state for each batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
