{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "# Seq2Seq Items\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.nn.rnn_cell import BasicLSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.rnn import DropoutWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 31411\n",
    "num_units = 256\n",
    "input_size = 128\n",
    "batch_size = 128\n",
    "source_sequence_length=40\n",
    "target_sequence_length=60\n",
    "encoder_type = 'bidirectional'\n",
    "decoder_type = 'attention' # could be basic or attention\n",
    "sentences_to_read = 292275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), (',', 3), ('.', 4), ('và', 5), ('tôi', 6), ('là', 7), ('một', 8), ('những', 9)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'và'), (6, 'tôi'), (7, 'là'), (8, 'một'), (9, 'những')]\n",
      "\t Vocabulary size:  31411\n",
      "Target\n",
      "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), (',', 3), ('.', 4), ('the', 5), ('and', 6), ('to', 7), ('of', 8), ('a', 9)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'the'), (6, 'and'), (7, 'to'), (8, 'of'), (9, 'a')]\n",
      "\t Vocabulary size:  31411\n"
     ]
    }
   ],
   "source": [
    "# Loading Vocabulary from vocabulary files\n",
    "src_dictionary = dict()\n",
    "with open('vocab.vi.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        src_dictionary[line[:-1]] = len(src_dictionary) # Assign an ID to each word in dictionary, ID is currently the entry and the word is the key\n",
    "\n",
    "src_reverse_dictionary = dict(zip(src_dictionary.values(),src_dictionary.keys())) # Reverse order, now ID is the key and the word is the entry\n",
    "#Somehow zipping the two list rearranges the zip to be in order.\n",
    "\n",
    "print('Source')\n",
    "print('\\t',list(src_dictionary.items())[:10]) \n",
    "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(src_dictionary))\n",
    "\n",
    "tgt_dictionary = dict()\n",
    "with open('vocab.en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
    "\n",
    "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
    "\n",
    "print('Target')\n",
    "print('\\t',list(tgt_dictionary.items())[:10])\n",
    "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(tgt_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations (292125)\n",
      "( 0 ) VI:  khoa_học đằng_sau một tiêu_đề về khí_hậu\n",
      "\n",
      "( 0 ) EN:  rachel pike : the science behind a climate headline\n",
      "\n",
      "( 10000 ) VI:  vì thế việc chúng_tôi cần làm là lấy những phân_tử đó và cấy chúng vào các nơ - ron .\n",
      "\n",
      "( 10000 ) EN:  so what we need to do is to take these molecules and somehow install them in neurons .\n",
      "\n",
      "( 20000 ) VI:  nếu bạn không chỉ muốn ong_chúa , bạn cũng có_thể mua , thực_tế là , 1 gói 1.4 kg ong , đến bằng bưu_phẩm , và dĩ_nhiên , bưu_điện luôn_luôn quan_tâm khi họ nhận được , cái gói 1.4 kg ong của bạn\n",
      "\n",
      "( 20000 ) EN:  if you do not just want a queen , you can buy , actually , a three-pound package of bees , which comes in the mail , and of course , the postal office is always very concerned when they get , you know , your three-pound packages of bees .\n",
      "\n",
      "( 30000 ) VI:  và vì_vậy chường một thực_sự bắt đầu hôm_nay , với tất_cả chúng_ta , vì trong mỗi chúng_ta là sức_mạnh để sang bằng các thiếu_hụt cơ_hội và để đóng lại lỗ_hổng hy_vọng .\n",
      "\n",
      "( 30000 ) EN:  and so chapter one really begins today , with all of us , because within each of us is the power to equal those opportunity gaps and to close the hope gaps .\n",
      "\n",
      "( 40000 ) VI:  kéo chiếc nút .\n",
      "\n",
      "( 40000 ) EN:  pull the knot .\n",
      "\n",
      "( 50000 ) VI:  đó là 1 chiếc xe chạy trong thành_phố . bạn lái_xe và xuống xe .\n",
      "\n",
      "( 50000 ) EN:  it is a city car . you drive along . you get out .\n",
      "\n",
      "( 60000 ) VI:  tôi cảm_thấy xung_quanh mình như có những con quỷ tay_cầm dao_găm .\n",
      "\n",
      "( 60000 ) EN:  all around me i sensed evil beings poised with daggers .\n",
      "\n",
      "( 70000 ) VI:  và bạn thực_sự cầu_mong câu trả_lời là không , vì đó là những gì bạn sắp nói .\n",
      "\n",
      "( 70000 ) EN:  and you are really hoping the answer is no , because that is what you are about to say .\n",
      "\n",
      "( 80000 ) VI:  đối với tôi bây_giờ , càng nhiều thành_viên kết_nạp vào châu âu không đơn_giản là tạo thêm nhiều quyền_lực hơn cho brút - xen .\n",
      "\n",
      "( 80000 ) EN:  now , more europe for me is not simply giving more power to brussels .\n",
      "\n",
      "( 90000 ) VI:  nhưng qua khoa_học , chúng_ta vẫn có_thể biết được tương_đối tốt chuyện gì đang xảy ra ở mức_độ phân_tử .\n",
      "\n",
      "( 90000 ) EN:  but through science , we do have a fairly good idea of what is going on down at the molecular scale .\n",
      "\n",
      "( 100000 ) VI:  cảm ơn các bạn .\n",
      "\n",
      "( 100000 ) EN:  thank you .\n",
      "\n",
      "( 110000 ) VI:  chúng_ta cần những tàu_lặn tân_tiến .\n",
      "\n",
      "( 110000 ) EN:  we need new deep-diving submarines .\n",
      "\n",
      "( 120000 ) VI:  nhờ nó , cô tôi có_thể ngủ_ngon mà không phải lo_lắng về việc ông đi lang_thang đâu_đây .\n",
      "\n",
      "( 120000 ) EN:  that way , my aunt could sleep much better at night without having to worry about my grandfather is wandering .\n",
      "\n",
      "( 130000 ) VI:  ch : và bạn cứ lăn cho đến khi dừng lại như_thể ai đó đã ném con_tàu xuống mặt_đất và nó cứ nhào_lộn lên xuống như_vậy nhưng bạn đã sẵn_sàng cho nó chỗ_ngồi của bạn được thiết_kế rất đặc_thù bạn biết bộ_phận giảm_xóc sẽ hoạt_động thế_nào\n",
      "\n",
      "( 130000 ) EN:  ch : and you roll to a stop as if someone threw your spaceship at the ground and it tumbles end over end , but you are ready for it you are in a custom-built seat , you know how the shock absorber works .\n",
      "\n",
      "( 140000 ) VI:  và cùng_với guillermo cecchi , người anh_em chung nhóm dự_án , chúng_tôi đã tìm_hiểu việc này .\n",
      "\n",
      "( 140000 ) EN:  and with guillermo cecchi ,  who has been my brother in this adventure ,  we took on this task . \n",
      "\n",
      "( 150000 ) VI:  fei fei li : đây là một bé gái ba tuổi đang miêu_tả những gì mà em nhìn_thấy trong loạt hình .\n",
      "\n",
      "( 150000 ) EN:  fei-fei li: this is a three-year-old child describing what she sees in a series of photos . \n",
      "\n",
      "( 160000 ) VI:  thế nên , toán_học nói_là trong 37 % cơ_hội hẹn_hò đầu_tiên , bạn không nên coi bất_cứ ai là tiềm_năng cho hôn_nhân nghiêm_túc .\n",
      "\n",
      "( 160000 ) EN:  so the math says then that what you should do in the first 37 percent of your dating window ,  you should just reject everybody as serious marriage potential . \n",
      "\n",
      "( 170000 ) VI:  rayan thay_đổi tình thế_và đưa chính mình xuống trình của tôi .\n",
      "\n",
      "( 170000 ) EN:  rayan reframed the situation and brought himself down to my level . \n",
      "\n",
      "( 180000 ) VI:  và thứ gì cần thêm , tôi sẽ trả cho anh khi tôi trở_lại . \"\n",
      "\n",
      "( 180000 ) EN:  and whatever else is needed ,  i will provide it and pay for it when i return .\" \n",
      "\n",
      "( 190000 ) VI:  tôi vui_mừng nói , \" phải thế_chứ . \"\n",
      "\n",
      "( 190000 ) EN:  i said ,   \" now you're talking !\" \n",
      "\n",
      "( 200000 ) VI:  nó thực_sự diễn ra như thế_nào ?\n",
      "\n",
      "( 200000 ) EN:  how does this actually work ? \n",
      "\n",
      "( 210000 ) VI:  hoặc là những điều nhiệm màu tương_tự xảy ra .\n",
      "\n",
      "( 210000 ) EN:  if life has arisen on only one planet in the entire universe , \n",
      "\n",
      "( 220000 ) VI:  tôi bắt_đầu nhận ra rằng có rất nhiều cách làm ngoại_giao - - thật ngoại_giao , cũng_như kinh_doanh , là ngành kinh_doanh giải_quyết vấn_đề , nhưng từ \" tiến_bộ \" lại không có trong nghành ngoại_giao ; vì đó chỉ là trò_chơi tổng bằng_không và sự thực_dụng và các tổ_chức cổ_lỗ sĩ cứ thế tồn_tại hàng thế_hệ để làm những việc mà họ vẫn làm như_thế suốt .\n",
      "\n",
      "( 220000 ) EN:  and i began to realize that there are different ways of doing diplomacy -- that diplomacy ,  like business ,  is a business of solving problems ,  and yet the word innovation doesn't exist in diplomacy; it's all zero sum games and realpolitik and ancient institutions that have been there for generations and do things the same way they've always done things . \n",
      "\n",
      "( 230000 ) VI:  tôi phải nói với các bạn nghiêm_trọng hơn là chúng_tôi đã nhấn_chìm cả niềm tự_hào của chúng_tôi , chúng_tôi đã làm hư động_cơ .\n",
      "\n",
      "( 230000 ) EN:  we drowned our pride ,  i must tell you ,  which was really serious ,  and we seized the engine . \n",
      "\n",
      "( 240000 ) VI:  và những khối tương_tác này nhận_biết lẫn nhau .\n",
      "\n",
      "( 240000 ) EN:  and these interactive portraits are aware of each other . \n",
      "\n",
      "( 250000 ) VI:  đó là câu_hỏi lớn chúng_ta phải đối_mặt chỉ hai hay ba_tháng trước .\n",
      "\n",
      "( 250000 ) EN:  that was the big question that we were facing just two or three months ago . \n",
      "\n",
      "( 260000 ) VI:  trước_hết , thật không dễ để xác_định đâu là âm_đạo của bạn .\n",
      "\n",
      "( 260000 ) EN:  in the first place ,  it's not so easy to even find your vagina . \n",
      "\n",
      "( 270000 ) VI:  nó thật_sự là một kinh nghiệm thực_tế .\n",
      "\n",
      "( 270000 ) EN:  it's really a hands-on experience . \n",
      "\n",
      "( 280000 ) VI:  nơi này từ 1 thành_phố nguy_hiểm nhất trở_thành nơi an_toàn nhất brazil , và họ làm điều đó bằng cách giảm một_nửa trong việc thu_thập thông_tin , chỉ ra điểm_nóng , cải_cách cảnh_sát , và qua quá_trình đó , họ đã giảm đến 70 % lượng tội_phạm giết_người trong 10 năm .\n",
      "\n",
      "( 280000 ) EN:  it's gone from being brazil's most dangerous city to one of its safest ,  and it did this by doubling down on information collection ,  hot spot mapping ,  and police reform ,  and in the process ,  it dropped homicide by 70 percent in just over 10 years . \n",
      "\n",
      "( 290000 ) VI:  chúng tạo là nền_móng để tạo nên những kí_tự khác\n",
      "\n",
      "( 290000 ) EN:  they are the building blocks for you to create lots more characters . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading Sentences(English and Vietnamese)\n",
    "source_sent = []\n",
    "target_sent = []\n",
    "\n",
    "test_source_sent = []\n",
    "test_target_sent = []\n",
    "\n",
    "\n",
    "with open('train.en.txt', encoding='utf-8') as f_en, open('train.vi.txt', encoding='utf-8') as f_vi:\n",
    "    for l_i, (line,line_2) in enumerate(zip(f_vi,f_en)):\n",
    "        if (len(line)==1 or len(line_2)==1):\n",
    "            continue\n",
    "        source_sent.append(line)\n",
    "        target_sent.append(line_2)\n",
    "        if len(target_sent)>=sentences_to_read:\n",
    "            break\n",
    "\n",
    "assert len(source_sent)==len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
    "\n",
    "sentences_to_read = len(source_sent)\n",
    "print('Sample translations (%d)'%len(source_sent))\n",
    "for i in range(0,sentences_to_read,10000):\n",
    "    print('(',i,') VI: ', source_sent[i])\n",
    "    print('(',i,') EN: ', target_sent[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Source) Sentence mean length:  22.32731536157467\n",
      "(Source) Sentence stddev length:  15.228319460982052\n",
      "(Target) Sentence mean length:  23.9698622165169\n",
      "(Target) Sentence stddev length:  16.154302459360334\n"
     ]
    }
   ],
   "source": [
    "def split_to_tokens(sent,is_source):\n",
    "    #sent = sent.replace('-',' ')\n",
    "    sent = sent.replace(',',' ,')\n",
    "    sent = sent.replace('.',' .')\n",
    "    sent = sent.replace('\\n',' ') \n",
    "    \n",
    "    sent_toks = sent.split(' ')\n",
    "    for t_i, tok in enumerate(sent_toks):\n",
    "        if is_source:\n",
    "            if tok not in src_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "        else:\n",
    "            if tok not in tgt_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "    return sent_toks\n",
    "\n",
    "# Let us first look at some statistics of the sentences\n",
    "source_len = []\n",
    "source_mean, source_std = 0,0\n",
    "for sent in source_sent:\n",
    "    source_len.append(len(split_to_tokens(sent,True)))\n",
    "\n",
    "print('(Source) Sentence mean length: ', np.mean(source_len))\n",
    "print('(Source) Sentence stddev length: ', np.std(source_len))\n",
    "\n",
    "target_len = []\n",
    "target_mean, target_std = 0,0\n",
    "for sent in target_sent:\n",
    "    target_len.append(len(split_to_tokens(sent,False)))\n",
    "\n",
    "print('(Target) Sentence mean length: ', np.mean(target_len))\n",
    "print('(Target) Sentence stddev length: ', np.std(target_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs=[]\n",
    "train_outputs=[]\n",
    "\n",
    "train_inp_lengths=[]\n",
    "train_out_lengths=[]\n",
    "\n",
    "src_max_sent_length= 41\n",
    "tgt_max_sent_length= 61\n",
    "\n",
    "for s_i, (src_sent, tgt_sent) in enumerate(zip(source_sent,target_sent)):\n",
    "    src_sent_tokens= split_to_tokens(src_sent,True)\n",
    "    tgt_sent_tokens= split_to_tokens(tgt_sent,False)\n",
    "    \n",
    "    #Process sentences for batch training(ADD special tokens and MAKE sentences into SAME length)\n",
    "    #Source Language\n",
    "    ############################################\n",
    "    num_src_sent = []\n",
    "    for tok in src_sent_tokens:\n",
    "        num_src_sent.append(src_dictionary[tok])\n",
    "    \n",
    "    num_src_rvs= num_src_sent[::-1]\n",
    "    num_src_sent.insert(0,src_dictionary['<s>'])\n",
    "    \n",
    "    train_inp_lengths.append(min(src_max_sent_length, len(num_src_sent)+1))\n",
    "    \n",
    "    # append until the sentence reaches max length\n",
    "    if len(num_src_sent)<src_max_sent_length:\n",
    "        for i in range(src_max_sent_length-len(num_src_sent)):\n",
    "            num_src_sent.append(src_dictionary['</s>'])\n",
    "    \n",
    "    #else truncate sentence until it reaches max length\n",
    "    elif len(num_src_sent) > src_max_sent_length:\n",
    "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
    "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
    "    \n",
    "    train_inputs.append(num_src_sent)\n",
    "    ##############################################\n",
    "    \n",
    "    #Target Language\n",
    "    ##############################################\n",
    "    num_tgt_sent=[tgt_dictionary['</s>']]\n",
    "    for tok in tgt_sent_tokens:\n",
    "        num_tgt_sent.append(tgt_dictionary[tok])\n",
    "        \n",
    "    train_out_lengths.append(min(len(num_tgt_sent)+1,tgt_max_sent_length))\n",
    "    \n",
    "    # append until the sentence reaches max length\n",
    "    if len(num_tgt_sent)< tgt_max_sent_length:\n",
    "        for i in range(tgt_max_sent_length-len(num_tgt_sent)):\n",
    "            num_tgt_sent.append(tgt_dictionary['</s>'])\n",
    "            \n",
    "    #else truncate sentence until it reaches max length\n",
    "    elif len(num_tgt_sent) > tgt_max_sent_length:\n",
    "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
    "    assert len(num_tgt_sent)==tgt_max_sent_length,len(num_tgt_sent)\n",
    "    \n",
    "    train_outputs.append(num_tgt_sent)\n",
    "    ################################################\n",
    "    \n",
    "assert len(train_inputs)  == len(source_sent),\\\n",
    "        'Size of total bin elements: %d, Total sentences: %d'\\\n",
    "                %(len(train_inputs),len(source_sent))\n",
    "\n",
    "\n",
    "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
    "train_outputs = np.array(train_outputs, dtype=np.int32)\n",
    "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data\n",
      "['khoa_học', 'trong', 'tôi', 'có', 'cả']\n",
      "['đằng_sau', '4', 'muốn', 'những', 'hai']\n",
      "['một', 'phút', 'cho', 'dòng', 'đều']\n",
      "['tiêu_đề', '<unk>', 'các', 'trông', 'là']\n",
      "['về', ',', 'bạn', 'như', 'một']\n",
      "['khí_hậu', 'chuyên_gia', 'biết', 'thế_này', 'nhánh']\n",
      "['<unk>', 'hoá_học', 'về', 'khi', 'của']\n",
      "['</s>', 'khí_quyển', 'sự', 'bàn', 'cùng']\n",
      "['</s>', 'rachel', 'to_lớn', 'về', 'một']\n",
      "['</s>', 'pike', 'của', 'biến_đổi', 'lĩnh_vực']\n",
      "['</s>', 'giới_thiệu', 'những', 'khí_hậu', 'trong']\n",
      "['</s>', 'sơ_lược', 'nỗ_lực', '<unk>', 'ngành']\n",
      "['</s>', 'về', 'khoa_học', ',', 'khoa_học']\n",
      "['</s>', 'những', 'đã', 'và', 'khí_quyển']\n",
      "['</s>', 'nỗ_lực', 'góp_phần', 'như', '<unk>']\n",
      "['</s>', 'khoa_học', 'làm_nên', 'thế_này', '.']\n",
      "['</s>', 'miệt_mài', 'các', 'khi', '<unk>']\n",
      "['</s>', 'đằng_sau', 'dòng', 'nói', '</s>']\n",
      "['</s>', 'những', 'tít', 'về', '</s>']\n",
      "['</s>', 'tiêu_đề', 'bạn', 'chất_lượng', '</s>']\n",
      "['</s>', 'táo_bạo', 'thường', 'không_khí', '</s>']\n",
      "['</s>', 'về', 'thấy', 'hay', '</s>']\n",
      "['</s>', 'biến_đổi', 'trên', 'khói', '</s>']\n",
      "['</s>', 'khí_hậu', 'báo', 'bụi', '</s>']\n",
      "['</s>', '<unk>', '<unk>', '<unk>', '</s>']\n",
      "['</s>', ',', '.', '.', '</s>']\n",
      "['</s>', 'cùng', '<unk>', '<unk>', '</s>']\n",
      "['</s>', 'với', '</s>', '</s>', '</s>']\n",
      "['</s>', 'đoàn', '</s>', '</s>', '</s>']\n",
      "['</s>', 'nghiên_cứu', '</s>', '</s>', '</s>']\n",
      "['</s>', 'của', '</s>', '</s>', '</s>']\n",
      "['</s>', 'mình', '</s>', '</s>', '</s>']\n",
      "['</s>', '-', '</s>', '</s>', '</s>']\n",
      "['</s>', '-', '</s>', '</s>', '</s>']\n",
      "['</s>', 'hàng', '</s>', '</s>', '</s>']\n",
      "['</s>', 'ngàn', '</s>', '</s>', '</s>']\n",
      "['</s>', 'người', '</s>', '</s>', '</s>']\n",
      "['</s>', 'đã', '</s>', '</s>', '</s>']\n",
      "['</s>', 'cống_hiến', '</s>', '</s>', '</s>']\n",
      "['</s>', 'cho', '</s>', '</s>', '</s>']\n",
      "\n",
      "Target data batch (first time)\n",
      "['rachel', 'i', 'headlines', 'they', 'recently']\n",
      "['pike', 'would', 'that', 'are', 'the']\n",
      "[':', 'like', 'look', 'both', 'headlines']\n",
      "['the', 'to', 'like', 'two', 'looked']\n",
      "['science', 'talk', 'this', 'branches', 'like']\n",
      "['behind', 'to', 'when', 'of', 'this']\n",
      "['a', 'you', 'they', 'the', 'when']\n",
      "['climate', 'today', 'have', 'same', 'the']\n",
      "['headline', 'about', 'to', 'field', 'intergovernmental']\n",
      "['<unk>', 'the', 'do', 'of', 'panel']\n",
      "['</s>', 'scale', 'with', 'atmospheric', 'on']\n",
      "['</s>', 'of', 'climate', 'science', 'climate']\n",
      "['</s>', 'the', 'change', '<unk>', 'change']\n",
      "['</s>', 'scientific', '<unk>', '.', '<unk>']\n",
      "['</s>', 'effort', ',', '<unk>', ',']\n",
      "['</s>', 'that', 'and', '</s>', 'or']\n",
      "['</s>', 'goes', 'headlines', '</s>', 'ipcc']\n",
      "['</s>', 'into', 'that', '</s>', '<unk>']\n",
      "['</s>', 'making', 'look', '</s>', ',']\n",
      "['</s>', 'the', 'like', '</s>', 'put']\n",
      "['</s>', 'headlines', 'this', '</s>', 'out']\n",
      "['</s>', 'you', 'when', '</s>', 'their']\n",
      "['</s>', 'see', 'they', '</s>', 'report']\n",
      "['</s>', 'in', 'have', '</s>', 'on']\n",
      "['</s>', 'the', 'to', '</s>', 'the']\n",
      "['</s>', 'paper', 'do', '</s>', 'state']\n",
      "['</s>', '<unk>', 'with', '</s>', 'of']\n",
      "['</s>', '.', 'air', '</s>', 'understanding']\n",
      "['</s>', '<unk>', 'quality', '</s>', 'of']\n",
      "['</s>', '</s>', 'or', '</s>', 'the']\n",
      "['</s>', '</s>', 'smog', '</s>', 'atmospheric']\n",
      "['</s>', '</s>', '<unk>', '</s>', 'system']\n",
      "['</s>', '</s>', '.', '</s>', '<unk>']\n",
      "['</s>', '</s>', '<unk>', '</s>', '.']\n",
      "['</s>', '</s>', '</s>', '</s>', '<unk>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "class DataGeneratorMT(object):\n",
    "    def __init__(self,batch_size,num_unroll,is_source):\n",
    "        self._batch_size= batch_size\n",
    "        self._num_unroll= num_unroll\n",
    "        self._cursor= [0 for offset in range (self._batch_size)]\n",
    "        \n",
    "        self._src_word_embeddings = np.load('vi_embeddings.npy')\n",
    "        self._tgt_word_embeddings = np.load('en_embeddings.npy')\n",
    "        \n",
    "        self._sent_ids = None\n",
    "        self._is_source =  is_source\n",
    "        \n",
    "    def next_batch(self,sent_ids, first_set):\n",
    "        if self._is_source:\n",
    "            max_sent_length= src_max_sent_length\n",
    "        else:\n",
    "            max_sent_length= tgt_max_sent_length\n",
    "        batch_label_ind= []\n",
    "        \n",
    "        batch_data= np.zeros((self._batch_size),dtype= np.float32)\n",
    "        batch_labels= np.zeros((self._batch_size),dtype= np.float32)\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            sent_id= sent_ids[b]\n",
    "            \n",
    "            if self._is_source:\n",
    "                sent_text= train_inputs[sent_id]\n",
    "                \n",
    "                batch_data[b]= sent_text[self._cursor[b]]\n",
    "                batch_labels[b]= sent_text[self._cursor[b]+1]\n",
    "            else:\n",
    "                sent_text= train_outputs[sent_id]\n",
    "                \n",
    "                batch_data[b]= sent_text[self._cursor[b]]\n",
    "                batch_labels[b]= sent_text[self._cursor[b]+1]\n",
    "            self._cursor[b]= (self._cursor[b]+1)%(max_sent_length-1)\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def unroll_batches(self,sent_ids):\n",
    "        \n",
    "        if sent_ids is not None:\n",
    "            self._sent_ids= sent_ids\n",
    "            self._cursor= [0 for _ in range(self._batch_size)]\n",
    "            \n",
    "        unroll_data,unroll_labels=[],[]\n",
    "        inp_lengths= None\n",
    "        \n",
    "        for ui in range(self._num_unroll):\n",
    "            \n",
    "            data,labels= self.next_batch(self._sent_ids,False)\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            inp_lengths= train_inp_lengths[sent_ids]\n",
    "        \n",
    "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if the implementation correct        \n",
    "dg= DataGeneratorMT(batch_size=5,num_unroll=40,is_source=True)\n",
    "\n",
    "inp= []\n",
    "for i in range(5):\n",
    "    inp.append(i)\n",
    "\n",
    "u_data, u_labels,_,_= dg.unroll_batches(inp)\n",
    "print('Source data')\n",
    "for _, lbl in zip(u_data,u_labels):\n",
    "     print([src_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "    \n",
    "dg= DataGeneratorMT(batch_size=5,num_unroll=60,is_source=False)\n",
    "u_data, u_labels,_,_= dg.unroll_batches([0,2,3,4,5])\n",
    "\n",
    "print('\\nTarget data batch (first time)')\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Encoding and Decoding mask layer\n",
    "tf.reset_default_graph()\n",
    "\n",
    "enc_train_inputs= []\n",
    "dec_train_inputs= []\n",
    "\n",
    "#Embedding Layer, received from pre-built word2vec embedding\n",
    "encoder_emb_layer = tf.convert_to_tensor(np.load('vi_embeddings.npy'),dtype=tf.float32)\n",
    "decoder_emb_layer = tf.convert_to_tensor(np.load('en_embeddings.npy'),dtype=tf.float32)\n",
    "\n",
    "\n",
    "#Defined unrolled training inputs\n",
    "for ui in range(source_sequence_length):\n",
    "    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size], name='enc_train_inputs_%d'%ui))\n",
    "    \n",
    "dec_train_labels=[]\n",
    "dec_label_masks= []\n",
    "\n",
    "for ui in range(target_sequence_length):\n",
    "    dec_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size], name='dec_train_inputs_%d'%ui))\n",
    "    dec_train_labels.append(tf.placeholder(tf.int32, shape=[batch_size], name='dec_train_inputs_%d'%ui))\n",
    "    dec_label_masks.append(tf.placeholder(tf.float32, shape=[batch_size], name='dec_train_inputs_%d'%ui))\n",
    "    \n",
    "encoder_emb_inp=[tf.nn.embedding_lookup(encoder_emb_layer,src) for src in enc_train_inputs]\n",
    "encoder_emb_inp= tf.stack(encoder_emb_inp)\n",
    "\n",
    "\n",
    "decoder_emb_inp=[tf.nn.embedding_lookup(decoder_emb_layer,src) for src in dec_train_inputs]  \n",
    "decoder_emb_inp= tf.stack(decoder_emb_inp)\n",
    "\n",
    "enc_train_inp_lengths= tf.placeholder(tf.int32, shape=[batch_size], name= \"train_input_lengths\")\n",
    "dec_train_inp_lengths= tf.placeholder(tf.int32, shape=[batch_size], name= \"train_output_lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\cuong\\appdata\\local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From c:\\users\\cuong\\appdata\\local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Tensor(\"transpose:0\", shape=(128, 40, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define Encoder\n",
    "forward_cell= BasicLSTMCell(num_units)\n",
    "backward_cell= BasicLSTMCell(num_units)\n",
    "\n",
    "forward_cell = DropoutWrapper(forward_cell, input_keep_prob = 0.8)\n",
    "backward_cell = DropoutWrapper(backward_cell, input_keep_prob = 0.8)\n",
    "\n",
    "initial_state_fw= forward_cell.zero_state(batch_size, dtype= tf.float32)\n",
    "initial_state_bw= backward_cell.zero_state(batch_size, dtype= tf.float32)\n",
    "\n",
    "bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, encoder_emb_inp, initial_state_fw=initial_state_fw,\n",
    "    initial_state_bw=initial_state_bw,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True)\n",
    "\n",
    "encoder_outputs = tf.concat(bi_outputs,-1)\n",
    "encoder_outputs= tf.transpose(encoder_outputs,[1,0,2])\n",
    "\n",
    "print(encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 128, 31411) dtype=float32>, sample_id=<tf.Tensor 'decoder/TensorArrayStack_1/TensorArrayGatherV3:0' shape=(?, 128) dtype=int32>)\n"
     ]
    }
   ],
   "source": [
    "#Define Decoder\n",
    "cells = [BasicLSTMCell(num_units),BasicLSTMCell(num_units)]\n",
    "decoder_cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "projection_layer= Dense(units=vocab_size,use_bias=True)\n",
    "\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, [tgt_max_sent_length-1 for _ in range(batch_size)], time_major=True)\n",
    "\n",
    "\n",
    "# Implement Bahdanau Attention\n",
    "# @attention_states=@memory in documentation  [batch_size, max_time, num_units]\n",
    "\n",
    "attention_states = encoder_outputs\n",
    "attention_mechanism= tf.contrib.seq2seq.LuongAttention(num_units, attention_states,\n",
    "                                                          scale=True)\n",
    "\n",
    "\n",
    "decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,attention_layer_size=num_units)\n",
    "\n",
    "initial_state= decoder_cell.zero_state(dtype=tf.float32, batch_size= batch_size).clone(cell_state=encoder_state)\n",
    "\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, initial_state,\n",
    "        output_layer=projection_layer)\n",
    "\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True,\n",
    "    swap_memory=True\n",
    ")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder/TensorArrayStack_1/TensorArrayGatherV3:0\", shape=(?, 128), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "logits= outputs.rnn_output\n",
    "\n",
    "cross_entropy= tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dec_train_labels, logits=logits)\n",
    "loss = (tf.reduce_sum(cross_entropy*tf.stack(dec_label_masks) / (batch_size*target_sequence_length)))\n",
    "\n",
    "train_prediction = outputs.sample_id\n",
    "print(train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Optimizer\n"
     ]
    }
   ],
   "source": [
    "print('Defining Optimizer')\n",
    "\n",
    "\n",
    "global_step= tf.Variable(0,trainable=False)\n",
    "inc_gstep= tf.assign(global_step,global_step+1)\n",
    "learning_rate= tf.train.exponential_decay(0.001, global_step,decay_steps=20,decay_rate=0.9,staircase=True)\n",
    "\n",
    "with tf.variable_scope('Adam'):\n",
    "    adam_optimizer= tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "adam_gradients, v= zip(*adam_optimizer.compute_gradients(loss))\n",
    "adam_gradients, _ = tf.clip_by_global_norm(adam_gradients,25.0)\n",
    "adam_optimize= adam_optimizer.apply_gradients(zip(adam_gradients,v))\n",
    "\n",
    "with tf.variable_scope('AdaDelta'):\n",
    "    ada_optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "\n",
    "ada_gradients, v = zip(*ada_optimizer.compute_gradients(loss))\n",
    "ada_gradients, _ = tf.clip_by_global_norm(ada_gradients, 25.0)\n",
    "ada_optimize = ada_optimizer.apply_gradients(zip(ada_gradients, v))\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver= tf.train.Saver()\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.mkdir('logs')\n",
    "log_dir = 'logs'\n",
    "\n",
    "bleu_scores_over_time = []\n",
    "loss_over_time = []\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "src_word_embeddings = np.load('vi_embeddings.npy')\n",
    "tgt_word_embeddings = np.load('en_embeddings.npy')\n",
    "\n",
    "# Defining data generators\n",
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=source_sequence_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=target_sequence_length,is_source=False)\n",
    "\n",
    "num_steps = 5000\n",
    "avg_loss = 0\n",
    "\n",
    "#writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "#scalar_summary = tf.summary.scalar('Loss: ', loss)\n",
    "\n",
    "print('Started Training')\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # input_sizes for each bin: [40]\n",
    "    # output_sizes for each bin: [60]\n",
    "    print('.',end='')\n",
    "    if (step+1)%100==0:\n",
    "        print('')\n",
    "        \n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "    # ====================== ENCODER DATA COLLECTION ================================================\n",
    "    \n",
    "    eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    feed_dict = {}\n",
    "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):            \n",
    "        feed_dict[enc_train_inputs[ui]] = dat                \n",
    "    \n",
    "    # ====================== DECODER DATA COLLECITON ===========================\n",
    "    # First step we change the ids in a batch\n",
    "    du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "     \n",
    "    feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "        feed_dict[dec_train_inputs[ui]] = dat\n",
    "        feed_dict[dec_train_labels[ui]] = lbl\n",
    "        feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)\n",
    "    \n",
    "    # ======================= OPTIMIZATION ==========================\n",
    "    if step < 10000:\n",
    "        _,l,tr_pred = sess.run([adam_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    else:\n",
    "        _,l,tr_pred = sess.run([ada_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "    # writer.add_summary(summary, step)    \n",
    "    tr_pred = tr_pred.flatten()\n",
    "    # print(len(feed_dict[dec_train_inputs[1]]))\n",
    "        \n",
    "    if (step+1)%250==0:  \n",
    "        bleu_labels, bleu_preds = [],[]\n",
    "        print('Step ',step+1)\n",
    "\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[::batch_size].tolist():\n",
    "            \n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            bleu_labels.append(tgt_reverse_dictionary[w])\n",
    "            \n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "                      \n",
    "        print(print_str)\n",
    "        print()\n",
    "        \n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[::batch_size].tolist():\n",
    "            \n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            bleu_preds.append(tgt_reverse_dictionary[w])\n",
    "            \n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "                \n",
    "        print(print_str)\n",
    "        print()\n",
    "        \n",
    "        print('BLEU: ')\n",
    "        print(nltk.translate.bleu_score.sentence_bleu([bleu_labels], bleu_preds))\n",
    "       \n",
    "        print('\\n')  \n",
    "        \n",
    "        bleu_labels, bleu_preds = [],[]\n",
    "        \n",
    "        \n",
    "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
    "            \n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            bleu_labels.append(tgt_reverse_dictionary[w])\n",
    "            \n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "                \n",
    "        print(print_str)\n",
    "        print()\n",
    "        \n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[rand_idx::batch_size].tolist():\n",
    "            \n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            bleu_preds.append(tgt_reverse_dictionary[w])\n",
    "            \n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "        print()\n",
    "        \n",
    "        print('BLEU: ')\n",
    "        print(nltk.translate.bleu_score.sentence_bleu([bleu_labels], bleu_preds))\n",
    "        print()  \n",
    "        \n",
    "    avg_loss += l\n",
    "    \n",
    "    #sess.run(reset_train_state) # resetting hidden state for each batchb \n",
    "    \n",
    "    if (step+1)%500==0:\n",
    "        print('============= Step ', str(step+1), ' =============')\n",
    "        print('\\t Loss: ',avg_loss/500.0)\n",
    "        \n",
    "        loss_over_time.append(avg_loss/500.0)\n",
    "             \n",
    "        avg_loss = 0.0\n",
    "        sess.run(inc_gstep)\n",
    "    \n",
    "    #if(step)%2000==0:\n",
    "        #saver.save(sess,'/NMT/models/Attention-Bi-Tokenized-292K-ViEn-B64-Luong',global_step=step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
